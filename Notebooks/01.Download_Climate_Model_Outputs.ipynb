{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access Coupled Model Intercomparison Project (CMIP) Data\n",
    "This notebook uses google cloud file storage to access outputs from the [Coupled Model Intercomparison Project 6](https://www.wcrp-climate.org/wgcm-cmip/wgcm-cmip6). Data can also be accessed at this [portal](https://esgf-node.llnl.gov/projects/cmip6/). This is the most involved part of the data collection portion of this project. We will compare the climate projections to historical temperature variability from weather stations in New Orleans. To see how gumbo weather has changed in the past 60 years, and will change 60 years in the future.  \n",
    "\n",
    "Climate models are run in ensembles. Each model, operated by different groups around the world, has slightly different physics. These models and their internal variability are compared to one another and averaged to produce a projection.  \n",
    "\n",
    "## Choosing a Scenario\n",
    "While each model contains different mathematical representations of the physics of earth's climate system, they depend on similar input \"forcings\", a stimulus to which the climate system should respond to. For earth surface temperature, this is almost entirely dependent on CO$_2$ concentrations in the atmosphere, which is completely dependent on future choices humanity makes about CO$_2$ emissions. The [Intergovernmental Panel on Climate Change](https://www.ipcc.ch) (IPCC) has outlined a few **scenarios** representing paths humanity might follow in our choices to curb fossil fuel emissions.  \n",
    "![](Images/rcps.jpg) <div style='text-align: right'> </div> \n",
    "Source: [Wikipedia: Relative Concentration Pathways](https://en.wikipedia.org/wiki/Representative_Concentration_Pathway)  \n",
    "<br>\n",
    "The above plot shows a few ways atmospheric CO$_2$ concentration might evolve over the coming century. They used to be called [Relative Concentration Pathways](https://en.wikipedia.org/wiki/Representative_Concentration_Pathway) (RCPs), but now these are integrated into the larger framework of a models that include population change, economic growth, education, urbanisation and the rate of technological development called [Shared Socioeconomic Pathways](https://www.carbonbrief.org/explainer-how-shared-socioeconomic-pathways-explore-future-climate-change) (SSPs). The numbers in these scenarios (2.6, 4.5, 6.0, 8.5) represent the power of [radiative forcing](https://en.wikipedia.org/wiki/Radiative_forcing) (watts/meter$^2$), which includes the effect of greenhouse gases, and today is estimated at 1.6 watts/meter$^2$. RCP 8.5/SSP 5 represents the most severe scenario, with no efforts to curb emissions. RCP 2.6/SSP 1 shows a path in which humanity takes a \"very stringent\" approach to reducing emissions, and is the path required to keep warming below the IPCC goal of 2 C.\n",
    "#### We choose to compare historical temperature variability to a scenario with intermediate severity, **RCP 4.5/SSP 2**  \n",
    "<br>\n",
    "In this scenario, CO2 emissions peak in 2045, and decline afterwards. Atmopsheric CO$_2$ concentrations stabilize by 2060. Total global warming in the situation is estimated between 2.5-3 &deg C.    \n",
    "<br><br>\n",
    "RCP 4.5/SSP 2 is an appropriate choice because it is close to the path humanity is currently on.\n",
    "\n",
    "## Code\n",
    "Below is the code required to pull down the outputs from climate models. Generally, it uses google cloud file storage to access the data, x-array and dask to contain and structure it, x-agg to slice it down to the pixel nearest New Orleans, and then unwraps it using dask. \n",
    "### Import Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xarray as xr\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cftime\n",
    "from tqdm.notebook import tqdm\n",
    "import re\n",
    "from operator import itemgetter\n",
    "import intake\n",
    "import gcsfs\n",
    "import os\n",
    "import warnings \n",
    "import xagg as xa\n",
    "import dask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Experiments, Variables, and Times\n",
    "We are going to pull down data for SSPs 2/RCP 4.5 and models which \"hindcast\" historical climate variability. Historical variability created by models is important in providing a benchmark for the warming predicted by the same model. For each simulation, you get a temperature difference from the models historical value, representing a temperature change. This is preferred to literally interpretting temperatures from the model output.  \n",
    "<br>\n",
    "Our variable is \"tas\" which stands for \"temperature of air at the surface.\" For historical we pull down 1985-2014 and we pull down all the data from 2020-2100 for the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params_all = [{'experiment_id':'historical','table_id':'day','variable_id':'tas','member_id':'r1i1p1f1'},\n",
    "                   {'experiment_id':'ssp245','table_id':'day','variable_id':'tas','member_id':'r1i1p1f1'}]\n",
    "subset_params = {'lat':[28,32],'lon':[-92,-88],\n",
    "                  'time':{'historical':['1985-01-01','2014-12-31'],\n",
    "                          'ssp245':['2020-01-01','2080-12-31']}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set up Google Cloud Storage access\n",
    "This code gets us a key to access the files and generates a table with metadata for the experiments. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Access google cloud storage links\n",
    "fs = gcsfs.GCSFileSystem(token='anon', access='read_only')\n",
    "# Get info about CMIP6 datasets\n",
    "cmip6_datasets = pd.read_csv('https://storage.googleapis.com/cmip6/cmip6-zarr-consolidated-stores.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = data_params_all[0]\n",
    "cmip6_sub = cmip6_datasets.query(' and '.join([k+\" == '\"+data_params[k]+\"'\" \n",
    "                                               for k in data_params.keys() \n",
    "                                               if k != 'other']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask.config.set({\"array.slicing.split_large_chunks\": False});"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use GCFS to Access Data\n",
    "Credit to [Kevin Schwarzwald](https://iri.columbia.edu/contact/staff-directory/kevin-schwarzwald/). I adapted this code to run from his [GitHub repository on climate model downloads](https://github.com/ks905383/climate-downloads).<br>\n",
    "<br>\n",
    "**Warning**: This block takes a couple minutes to run. There will be a lot of stuff printed in the notebook and some warnings thrown, but don't worry about them. Two progress bars will appear, one for SSP245 and another for the historical models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "200dcd7cb7604fafb9fb3845f55fdcd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/42 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing GFDL-CM4!\n",
      "GFDL-CM4 processed!\n",
      "processing GFDL-CM4!\n",
      "GFDL-CM4 processed!\n",
      "processing BCC-CSM2-MR!\n",
      "BCC-CSM2-MR processed!\n",
      "processing AWI-CM-1-1-MR!\n",
      "AWI-CM-1-1-MR processed!\n",
      "processing BCC-ESM1!\n",
      "BCC-ESM1 processed!\n",
      "processing CESM2-WACCM!\n",
      "CESM2-WACCM processed!\n",
      "processing CESM2!\n",
      "CESM2 processed!\n",
      "processing SAM0-UNICON!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/558_c385167_rhq6m2ykzd_40000gn/T/ipykernel_10391/1613398894.py:31: UserWarning: Model SAM0-UNICON has an unsorted time dimension.\n",
      "  warnings.warn('Model '+ds.source_id+' has an unsorted time dimension.')\n",
      "/Users/danielbabin/opt/miniconda3/envs/geospatial/lib/python3.8/site-packages/xarray/core/indexing.py:1227: PerformanceWarning: Slicing with an out-of-order index is generating 304 times more chunks\n",
      "  return self.array[key]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAM0-UNICON processed!\n",
      "processing CanESM5!\n",
      "CanESM5 processed!\n",
      "processing INM-CM4-8!\n",
      "INM-CM4-8 processed!\n",
      "processing MRI-ESM2-0!\n",
      "MRI-ESM2-0 processed!\n",
      "processing INM-CM5-0!\n",
      "INM-CM5-0 processed!\n",
      "processing IPSL-CM6A-LR!\n",
      "IPSL-CM6A-LR processed!\n",
      "processing MPI-ESM-1-2-HAM!\n",
      "MPI-ESM-1-2-HAM processed!\n",
      "processing MPI-ESM1-2-LR!\n",
      "MPI-ESM1-2-LR processed!\n",
      "processing MPI-ESM1-2-HR!\n",
      "MPI-ESM1-2-HR processed!\n",
      "processing GFDL-ESM4!\n",
      "GFDL-ESM4 processed!\n",
      "processing NESM3!\n",
      "NESM3 processed!\n",
      "processing NorESM2-LM!\n",
      "NorESM2-LM processed!\n",
      "processing FGOALS-g3!\n",
      "FGOALS-g3 processed!\n",
      "processing MIROC6!\n",
      "MIROC6 processed!\n",
      "processing FGOALS-f3-L!\n",
      "FGOALS-f3-L processed!\n",
      "processing ACCESS-CM2!\n",
      "ACCESS-CM2 processed!\n",
      "processing NorESM2-MM!\n",
      "NorESM2-MM processed!\n",
      "processing ACCESS-ESM1-5!\n",
      "ACCESS-ESM1-5 processed!\n",
      "processing CESM2-WACCM-FV2!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/558_c385167_rhq6m2ykzd_40000gn/T/ipykernel_10391/1613398894.py:31: UserWarning: Model CESM2-WACCM-FV2 has an unsorted time dimension.\n",
      "  warnings.warn('Model '+ds.source_id+' has an unsorted time dimension.')\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CESM2-WACCM-FV2 processed!\n",
      "processing CESM2-FV2!\n",
      "CESM2-FV2 processed!\n",
      "processing KIOST-ESM!\n",
      "KIOST-ESM processed!\n",
      "processing IITM-ESM!\n",
      "IITM-ESM processed!\n",
      "processing AWI-ESM-1-1-LR!\n",
      "AWI-ESM-1-1-LR processed!\n",
      "processing EC-Earth3-Veg-LR!\n",
      "EC-Earth3-Veg-LR processed!\n",
      "processing EC-Earth3-Veg!\n",
      "EC-Earth3-Veg processed!\n",
      "processing EC-Earth3!\n",
      "EC-Earth3 processed!\n",
      "processing KACE-1-0-G!\n",
      "KACE-1-0-G processed!\n",
      "processing CMCC-CM2-SR5!\n",
      "CMCC-CM2-SR5 processed!\n",
      "processing EC-Earth3-AerChem!\n",
      "EC-Earth3-AerChem processed!\n",
      "processing TaiESM1!\n",
      "TaiESM1 processed!\n",
      "processing NorCPM1!\n",
      "NorCPM1 processed!\n",
      "processing IPSL-CM5A2-INCA!\n",
      "IPSL-CM5A2-INCA processed!\n",
      "processing CMCC-CM2-HR4!\n",
      "CMCC-CM2-HR4 processed!\n",
      "processing EC-Earth3-CC!\n",
      "EC-Earth3-CC processed!\n",
      "processing CMCC-ESM2!\n",
      "CMCC-ESM2 processed!\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "996d6d769baf4a90a35b8cd506468a82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/29 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processing GFDL-CM4!\n",
      "GFDL-CM4 processed!\n",
      "processing GFDL-CM4!\n",
      "GFDL-CM4 processed!\n",
      "processing GFDL-ESM4!\n",
      "GFDL-ESM4 processed!\n",
      "processing BCC-CSM2-MR!\n",
      "BCC-CSM2-MR processed!\n",
      "processing CanESM5!\n",
      "CanESM5 processed!\n",
      "processing AWI-CM-1-1-MR!\n",
      "AWI-CM-1-1-MR processed!\n",
      "processing MRI-ESM2-0!\n",
      "MRI-ESM2-0 processed!\n",
      "processing INM-CM4-8!\n",
      "INM-CM4-8 processed!\n",
      "processing IPSL-CM6A-LR!\n",
      "IPSL-CM6A-LR processed!\n",
      "processing INM-CM5-0!\n",
      "INM-CM5-0 processed!\n",
      "processing MPI-ESM1-2-LR!\n",
      "MPI-ESM1-2-LR processed!\n",
      "processing MPI-ESM1-2-HR!\n",
      "MPI-ESM1-2-HR processed!\n",
      "processing NESM3!\n",
      "NESM3 processed!\n",
      "processing CESM2-WACCM!\n",
      "CESM2-WACCM processed!\n",
      "processing FGOALS-g3!\n",
      "FGOALS-g3 processed!\n",
      "processing MIROC6!\n",
      "MIROC6 processed!\n",
      "processing NorESM2-LM!\n",
      "NorESM2-LM processed!\n",
      "processing ACCESS-CM2!\n",
      "ACCESS-CM2 processed!\n",
      "processing NorESM2-MM!\n",
      "NorESM2-MM processed!\n",
      "processing KIOST-ESM!\n",
      "KIOST-ESM processed!\n",
      "processing EC-Earth3-Veg!\n",
      "EC-Earth3-Veg processed!\n",
      "processing EC-Earth3!\n",
      "EC-Earth3 processed!\n",
      "processing KACE-1-0-G!\n",
      "KACE-1-0-G processed!\n",
      "processing CMCC-CM2-SR5!\n",
      "CMCC-CM2-SR5 processed!\n",
      "processing IITM-ESM!\n",
      "IITM-ESM processed!\n",
      "processing EC-Earth3-Veg-LR!\n",
      "EC-Earth3-Veg-LR processed!\n",
      "processing EC-Earth3-CC!\n",
      "EC-Earth3-CC processed!\n",
      "processing CMCC-ESM2!\n",
      "CMCC-ESM2 processed!\n",
      "processing TaiESM1!\n",
      "TaiESM1 processed!\n"
     ]
    }
   ],
   "source": [
    "remove_leaps = True\n",
    "dss_out = dict()\n",
    "for data_params in data_params_all:\n",
    "    dss_out[data_params['experiment_id']] = dict()\n",
    "\n",
    "    cmip6_sub = cmip6_datasets.query(' and '.join([k+\" == '\"+data_params[k]+\"'\" \n",
    "                                                   for k in data_params.keys() \n",
    "                                                   if k != 'other']))\n",
    "        \n",
    "    for url in tqdm(cmip6_sub.zstore.values):\n",
    "        mod = re.split('\\/',url)[6]\n",
    "        print('processing '+mod+'!')\n",
    "        \n",
    "        \n",
    "        # Open dataset\n",
    "        ds = xr.open_zarr(fs.get_mapper(url),consolidated=True)\n",
    "\n",
    "        # Coerce all possible geospatial attribute names to simply 'lat' and 'lon'\n",
    "        try:\n",
    "            ds = ds.rename({'longitude':'lon','latitude':'lat'})\n",
    "        except: \n",
    "            pass\n",
    "        try:\n",
    "            ds = ds.rename({'nav_lon':'lon','nav_lat':'lat'})\n",
    "        except: \n",
    "            pass\n",
    "\n",
    "        # Sort by time, if not sorted \n",
    "        if 'time' in subset_params:\n",
    "            if (ds.time.values != np.sort(ds.time)).any():\n",
    "                warnings.warn('Model '+ds.source_id+' has an unsorted time dimension.')\n",
    "                ds = ds.sortby('time')\n",
    "            \n",
    "        # Now, save by the subsets desired in subset_params_all above\n",
    "        ds_tmp = xa.fix_ds(ds)\n",
    "        # Subset by time as set in subset_params\n",
    "        if 'time' in subset_params:\n",
    "            if (ds.time.max().dt.day==30) | (type(ds.time.values[0]) == cftime._cftime.Datetime360Day): \n",
    "                ds_tmp = (ds_tmp.sel(time=slice(subset_params['time'][data_params['experiment_id']][0],\n",
    "                                        re.sub('-31','-30',subset_params['time'][data_params['experiment_id']][1]))))\n",
    "            else:\n",
    "                ds_tmp = (ds_tmp.sel(time=slice(*subset_params['time'][data_params['experiment_id']])))\n",
    "\n",
    "        # Subset by space as set in subset_params\n",
    "        if 'lat' in subset_params.keys():\n",
    "            if not 'lat' in ds[data_params['variable_id']].dims:\n",
    "                ds_tmp = ds_tmp.where((ds_tmp.lat >= subset_params['lat'][0]) & (ds_tmp.lat <= subset_params['lat'][1]) &\n",
    "                 (ds_tmp.lon >= subset_params['lon'][0]) & (ds_tmp.lon <= subset_params['lon'][1]),drop=True)\n",
    "            else:\n",
    "                ds_tmp = (ds_tmp.sel(lat=slice(*subset_params['lat']),\n",
    "                                     lon=slice(*subset_params['lon'])))\n",
    "\n",
    "        # Output\n",
    "        dss_out[data_params['experiment_id']][mod] = ds_tmp\n",
    "\n",
    "        # Status update\n",
    "        print(mod+' processed!')\n",
    "        \n",
    "        del ds, ds_tmp\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subset to Closest Pixel to New Orleans\n",
    "This block does some geospatial slicing to get the pixel to New Orleans. The lat-lon of New Orleans is 30 N, 90 W. So this code just subtracts 30 from the latitude, 90 from the longitude, takes the absolute value, and finds the index of the minimum value. The lat and lon coordinates in each dataset represent the south and west corner of a box, which represents a pixel int the model space. Each model is gridded slightly different, meaning the pixels will be different sizes for different models. They average about 130 x 130 km (80 x 80 miles), but can be as big as 250 x 250 km (150 x 150 miles)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "for exp in dss_out:\n",
    "    for mod in dss_out[exp]:\n",
    "        dss_out[exp][mod] = dss_out[exp][mod].isel(lat=np.abs(dss_out[exp][mod].lat-30).argmin(),\n",
    "                                                    lon=np.abs(dss_out[exp][mod].lon-(-90)).argmin())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Simulations\n",
    "At the moment, everything has been loaded lazily using dask. That basically means that the computer knows where to look to get the data and what to do with it, but hasn't done it yet. Running the .compute() operation on a dask object will load the data and get some real numbers. But because the volume of data is so large, this will take a few hours. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SSP 2 4.5\n",
    "SSP245 is the middle of the road scenario, where we invest a minimum effort to curb emissions. We are basically on this path at the moment. We drop the KACE-1-0-G because it returned an empty dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=list(dss_out['ssp245'].keys())\n",
    "keys.remove('KACE-1-0-G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFDL-CM4\n",
      "GFDL-ESM4\n",
      "BCC-CSM2-MR\n",
      "CanESM5\n",
      "AWI-CM-1-1-MR\n",
      "MRI-ESM2-0\n",
      "INM-CM4-8\n",
      "IPSL-CM6A-LR\n",
      "INM-CM5-0\n",
      "MPI-ESM1-2-LR\n",
      "MPI-ESM1-2-HR\n",
      "NESM3\n",
      "CESM2-WACCM\n",
      "FGOALS-g3\n",
      "MIROC6\n",
      "NorESM2-LM\n",
      "ACCESS-CM2\n",
      "NorESM2-MM\n",
      "KIOST-ESM\n",
      "EC-Earth3-Veg\n",
      "EC-Earth3\n",
      "CMCC-CM2-SR5\n",
      "IITM-ESM\n",
      "EC-Earth3-Veg-LR\n",
      "EC-Earth3-CC\n",
      "CMCC-ESM2\n",
      "TaiESM1\n"
     ]
    }
   ],
   "source": [
    "ssp245={}\n",
    "for key in keys:\n",
    "    ssp245[key]=dss_out['ssp245'][key]['tas'].compute()\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Historical\n",
    "It's necessary to draw down historical data for our projections. For each simulation, you get a temperature difference from the models historical value, representing a temperature change. This is preferred to literally interpretting temperatures from the model output. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys=list(dss_out['historical'].keys())\n",
    "keys.remove('KACE-1-0-G')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GFDL-CM4\n",
      "BCC-CSM2-MR\n",
      "AWI-CM-1-1-MR\n",
      "BCC-ESM1\n",
      "CESM2-WACCM\n",
      "CESM2\n",
      "SAM0-UNICON\n",
      "CanESM5\n",
      "INM-CM4-8\n",
      "MRI-ESM2-0\n",
      "INM-CM5-0\n",
      "IPSL-CM6A-LR\n",
      "MPI-ESM-1-2-HAM\n",
      "MPI-ESM1-2-LR\n",
      "MPI-ESM1-2-HR\n",
      "GFDL-ESM4\n",
      "NESM3\n",
      "NorESM2-LM\n",
      "FGOALS-g3\n",
      "MIROC6\n",
      "FGOALS-f3-L\n",
      "ACCESS-CM2\n",
      "NorESM2-MM\n",
      "ACCESS-ESM1-5\n",
      "CESM2-WACCM-FV2\n",
      "CESM2-FV2\n",
      "KIOST-ESM\n",
      "IITM-ESM\n",
      "AWI-ESM-1-1-LR\n",
      "EC-Earth3-Veg-LR\n",
      "EC-Earth3-Veg\n",
      "EC-Earth3\n",
      "CMCC-CM2-SR5\n",
      "EC-Earth3-AerChem\n",
      "TaiESM1\n",
      "NorCPM1\n",
      "IPSL-CM5A2-INCA\n",
      "CMCC-CM2-HR4\n",
      "EC-Earth3-CC\n",
      "CMCC-ESM2\n"
     ]
    }
   ],
   "source": [
    "historical={}\n",
    "for key in keys:\n",
    "    historical[key]=dss_out['historical'][key]['tas'].compute()\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure and Export\n",
    "This code restructures these dask/x-array objects into a simple table with a time series and exports them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes={}\n",
    "scenarios_keys=['historical','ssp245']\n",
    "scenarios_data={'historical':historical,\n",
    "                'ssp245':ssp245}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_path='/Users/danielbabin/GitHub/Gumbo_Weather/Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0d/558_c385167_rhq6m2ykzd_40000gn/T/ipykernel_10391/2231930991.py:9: RuntimeWarning: Converting a CFTimeIndex with dates from a non-standard calendar, 'noleap', to a pandas.DatetimeIndex, which uses dates from the standard calendar.  This may lead to subtle errors in operations that depend on the length of time between dates.\n",
      "  dataframes[scen]['date']=dataframes[scen].index.to_datetimeindex()\n"
     ]
    }
   ],
   "source": [
    "for scen in scenarios_keys:\n",
    "    experiments=list(scenarios_data[scen].keys())\n",
    "    dataframes[scen]=scenarios_data[scen][experiments[0]].to_dataframe()\n",
    "    dataframes[scen].rename(columns={'tasmax':experiments[0]})\n",
    "    for exp in scenarios_data[scen].keys():\n",
    "        values=scenarios_data[scen][exp].values\n",
    "        if len(values)==len(dataframes[scen]):\n",
    "            dataframes[scen][exp]=scenarios_data[scen][exp].values\n",
    "    dataframes[scen]['date']=dataframes[scen].index.to_datetimeindex()\n",
    "    dataframes[scen]=dataframes[scen].set_index('date')\n",
    "    dataframes[scen].to_csv(out_path+scen+'.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
